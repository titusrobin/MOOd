{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.splits import create_splits_scenes\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class NuScenesConesDataset:\n",
    "    def __init__(self, dataroot, version=\"v1.0-mini\", split=\"train\"):\n",
    "        self.nusc = NuScenes(version=version, dataroot=dataroot, verbose=True)\n",
    "        self.split = split\n",
    "\n",
    "        # Get scene names from splits\n",
    "        self.scene_names = create_splits_scenes()[split]\n",
    "\n",
    "        # Map scene names to scene tokens\n",
    "        self.scene_tokens = []\n",
    "        for scene in self.nusc.scene:\n",
    "            if scene[\"name\"] in self.scene_names:\n",
    "                self.scene_tokens.append(scene[\"token\"])\n",
    "\n",
    "    def prepare_dataset(self, output_dir, max_images=None):\n",
    "        \"\"\"\n",
    "        Prepare dataset in YOLO format with improved label handling\n",
    "        \"\"\"\n",
    "        # Create directories\n",
    "        images_dir = os.path.join(output_dir, \"images\", self.split)\n",
    "        labels_dir = os.path.join(output_dir, \"labels\", self.split)\n",
    "\n",
    "        os.makedirs(images_dir, exist_ok=True)\n",
    "        os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "        image_count = 0\n",
    "        skipped_count = 0\n",
    "\n",
    "        # Track all annotations\n",
    "        for scene_token in tqdm(\n",
    "            self.scene_tokens, desc=f\"Processing {self.split} scenes\"\n",
    "        ):\n",
    "            scene_rec = self.nusc.get(\"scene\", scene_token)\n",
    "            sample = self.nusc.get(\"sample\", scene_rec[\"first_sample_token\"])\n",
    "\n",
    "            while sample:\n",
    "                # Check if we've reached the maximum number of images\n",
    "                if max_images is not None and image_count >= max_images:\n",
    "                    print(f\"Reached maximum number of images ({max_images})\")\n",
    "                    break\n",
    "\n",
    "                # Get camera images\n",
    "                cam_front_data = self.nusc.get(\n",
    "                    \"sample_data\", sample[\"data\"][\"CAM_FRONT\"]\n",
    "                )\n",
    "\n",
    "                # Get image\n",
    "                img_path = os.path.join(self.nusc.dataroot, cam_front_data[\"filename\"])\n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"Warning: Image not found at {img_path}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not open image {img_path}: {e}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Get annotations\n",
    "                annotations = []\n",
    "                has_cones = False  # Flag to track if image has any cone annotations\n",
    "\n",
    "                for ann_token in sample[\"anns\"]:\n",
    "                    ann_rec = self.nusc.get(\"sample_annotation\", ann_token)\n",
    "                    if ann_rec[\"category_name\"] == \"movable_object.traffic_cone\":\n",
    "                        has_cones = True\n",
    "                        try:\n",
    "                            # Get 2D bbox in image coordinates\n",
    "                            bbox = self.nusc.get_box(ann_rec[\"token\"])\n",
    "                            corners_2d = self.nusc.box_to_keypoints(\n",
    "                                cam_front_data[\"token\"], bbox\n",
    "                            )\n",
    "\n",
    "                            # Convert to YOLO format (normalized coordinates)\n",
    "                            x_min, y_min = np.min(corners_2d, axis=0)\n",
    "                            x_max, y_max = np.max(corners_2d, axis=0)\n",
    "\n",
    "                            # Skip if bbox is outside image bounds\n",
    "                            if (\n",
    "                                x_min >= img.width\n",
    "                                or x_max <= 0\n",
    "                                or y_min >= img.height\n",
    "                                or y_max <= 0\n",
    "                            ):\n",
    "                                continue\n",
    "\n",
    "                            # Clip coordinates to image bounds\n",
    "                            x_min = max(0, x_min)\n",
    "                            x_max = min(img.width, x_max)\n",
    "                            y_min = max(0, y_min)\n",
    "                            y_max = min(img.height, y_max)\n",
    "\n",
    "                            # YOLO format: <class> <x_center> <y_center> <width> <height>\n",
    "                            x_center = ((x_min + x_max) / 2) / img.width\n",
    "                            y_center = ((y_min + y_max) / 2) / img.height\n",
    "                            bbox_width = (x_max - x_min) / img.width\n",
    "                            bbox_height = (y_max - y_min) / img.height\n",
    "\n",
    "                            # Ensure values are within [0, 1]\n",
    "                            if (\n",
    "                                0 <= x_center <= 1\n",
    "                                and 0 <= y_center <= 1\n",
    "                                and 0 <= bbox_width <= 1\n",
    "                                and 0 <= bbox_height <= 1\n",
    "                            ):\n",
    "                                annotations.append(\n",
    "                                    f\"0 {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\"\n",
    "                                )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Error processing annotation: {e}\")\n",
    "                            continue\n",
    "\n",
    "                # Only save image if it has valid cone annotations\n",
    "                if has_cones and annotations:\n",
    "                    # Save image and labels\n",
    "                    img_filename = os.path.basename(cam_front_data[\"filename\"])\n",
    "                    label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
    "\n",
    "                    # Save image\n",
    "                    img_save_path = os.path.join(images_dir, img_filename)\n",
    "                    img.save(img_save_path)\n",
    "\n",
    "                    # Save labels\n",
    "                    label_save_path = os.path.join(labels_dir, label_filename)\n",
    "                    with open(label_save_path, \"w\") as f:\n",
    "                        f.write(\"\\n\".join(annotations))\n",
    "\n",
    "                    image_count += 1\n",
    "\n",
    "                    if image_count % 10 == 0:\n",
    "                        print(f\"Processed {image_count} images with cone annotations\")\n",
    "\n",
    "                # Move to next sample\n",
    "                if not sample[\"next\"]:\n",
    "                    break\n",
    "                sample = self.nusc.get(\"sample\", sample[\"next\"])\n",
    "\n",
    "        print(f\"\\nDataset preparation complete:\")\n",
    "        print(f\"- Total images processed with cone annotations: {image_count}\")\n",
    "        print(f\"- Images skipped: {skipped_count}\")\n",
    "\n",
    "        # Verify dataset\n",
    "        self._verify_dataset(images_dir, labels_dir)\n",
    "\n",
    "    def _verify_dataset(self, images_dir, labels_dir):\n",
    "        \"\"\"\n",
    "        Verify that the dataset was created correctly\n",
    "        \"\"\"\n",
    "        image_files = set(\n",
    "            os.path.splitext(f)[0]\n",
    "            for f in os.listdir(images_dir)\n",
    "            if f.endswith((\".jpg\", \".png\"))\n",
    "        )\n",
    "        label_files = set(\n",
    "            os.path.splitext(f)[0] for f in os.listdir(labels_dir) if f.endswith(\".txt\")\n",
    "        )\n",
    "\n",
    "        print(\"\\nDataset verification:\")\n",
    "        print(f\"- Number of images: {len(image_files)}\")\n",
    "        print(f\"- Number of label files: {len(label_files)}\")\n",
    "\n",
    "        # Check for mismatches\n",
    "        images_without_labels = image_files - label_files\n",
    "        labels_without_images = label_files - image_files\n",
    "\n",
    "        if images_without_labels:\n",
    "            print(f\"Warning: Found {len(images_without_labels)} images without labels\")\n",
    "        if labels_without_images:\n",
    "            print(f\"Warning: Found {len(labels_without_images)} labels without images\")\n",
    "\n",
    "        # Verify label format\n",
    "        invalid_labels = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            if label_file.endswith(\".txt\"):\n",
    "                with open(os.path.join(labels_dir, label_file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    for i, line in enumerate(lines):\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) != 5:\n",
    "                            invalid_labels.append((label_file, i + 1))\n",
    "\n",
    "        if invalid_labels:\n",
    "            print(f\"Warning: Found {len(invalid_labels)} invalid label entries\")\n",
    "            for label_file, line_num in invalid_labels[:5]:  # Show first 5 errors\n",
    "                print(f\"  - {label_file}:line {line_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.430 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train scenes: 100%|██████████| 6/6 [00:00<00:00, 273.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparation complete:\n",
      "- Total images processed with cone annotations: 0\n",
      "- Images skipped: 0\n",
      "\n",
      "Dataset verification:\n",
      "- Number of images: 0\n",
      "- Number of label files: 0\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.236 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val scenes: 100%|██████████| 4/4 [00:00<00:00, 258.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset preparation complete:\n",
      "- Total images processed with cone annotations: 0\n",
      "- Images skipped: 0\n",
      "\n",
      "Dataset verification:\n",
      "- Number of images: 0\n",
      "- Number of label files: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "dataroot = \"../data/v1.0-mini\"  # Replace with your NuScenes data path\n",
    "output_dir = \"../output_6\"  # Replace with your desired output path\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NuScenesConesDataset(dataroot, split=\"train\")\n",
    "train_dataset.prepare_dataset(\n",
    "    output_dir, max_images=100\n",
    ")  # Increased max_images for better training\n",
    "\n",
    "val_dataset = NuScenesConesDataset(dataroot, split=\"val\")\n",
    "val_dataset.prepare_dataset(output_dir, max_images=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
